# 4 ‚Äî LIMITES DO SISTEMA

> **Prop√≥sito**: Declarar explicitamente onde o sistema quebra.
> **Atualizar**: Ap√≥s testes de carga ou incidentes.

---

## 4.1 ‚Äî Limites de Capacidade

| Recurso | Limite Te√≥rico | Limite Real Estimado | Gargalo |
|---------|---------------|---------------------|---------|
| **Tenants ativos** | Ilimitado (RLS) | ~200 | Postgres connections + noisy neighbor |
| **Usu√°rios concurrent** | PgBouncer pool (~500 conn) | ~2.000 | Connection pool exhaustion |
| **Webhooks/hora** | Ilimitado (tabela) | ~10.000 | Advisory lock serialization + VACUUM pressure |
| **Mensagens WA/hora** | Ilimitado (tabela) | ~50.000 | wa_messages INSERT rate + Realtime fanout |
| **Realtime channels** | ~10.000 (Supabase) | ~500 concurrent | Channel fanout + server memory |
| **Edge Function concurrent** | ~100 (Supabase) | ~50 | Cold start + 60s timeout |
| **Storage per tenant** | Bucket limits | ~10GB | Supabase plan limits |
| **MV refresh time** | Depende de dados | <5s com <100k rows | Lock duration durante REFRESH |

---

## 4.2 ‚Äî Pontos de Quebra por Escala

### Com 10 tenants (ATUAL)
‚úÖ Sistema funciona bem. Bloat em `wa_webhook_events` √© a √∫nica anomalia vis√≠vel.

### Com 50 tenants
‚ö†Ô∏è **Primeiros sinais de stress**:
- Dashboard MVs levam mais tempo para refresh
- Connection pool come√ßa a saturar em hor√°rios de pico
- Realtime channels se aproximam do limite pr√°tico
- Cleanup jobs de pg_cron podem come√ßar a sobrecarregar

### Com 200 tenants
üî¥ **Degrada√ß√£o significativa**:
- Postgres come√ßa a recusar conex√µes em picos
- `wa_webhook_events` bloat atinge GBs
- Queries de `get_super_admin_metrics` (que faz COUNT em leads, profiles, clientes por tenant) se tornam lentas
- Realtime come√ßa a dropar conex√µes silenciosamente

### Com 1.000 tenants
üíÄ **Sistema inoper√°vel sem redesenho**:
- Postgres √∫nico n√£o suporta a carga de reads + writes
- `wa_messages` sem parti√ß√£o = scans em bilh√µes de rows
- `audit_logs` sem parti√ß√£o = imposs√≠vel de consultar
- Edge Functions com cold start = timeout em cascata
- SPA bundle size cresce com features = slow load

### Com 10.000+ tenants
üíÄüíÄ **Imposs√≠vel na arquitetura atual**. Necess√°rio:
- Read replicas (m√≠nimo 2)
- Sharding por tenant range ou geogr√°fico
- Queue externo (SQS/Redis Streams)
- CDN + edge caching
- Kubernetes ou serverless com auto-scaling real

---

## 4.3 ‚Äî Gargalos Espec√≠ficos

### Gargalo 1: Advisory Lock no Webhook Processing
```
wa_webhook_events INSERT ‚Üí pg_try_advisory_lock ‚Üí process batch ‚Üí release
```
- **Throughput**: 1 worker por vez (by design, para idempot√™ncia)
- **Limite**: Se processamento de 1 batch leva 5s, throughput m√°ximo = 12 batches/min
- **Mitiga√ß√£o**: Batch size ajust√°vel, mas limitado pelo wall-clock de 60s da Edge Function

### Gargalo 2: `get_super_admin_metrics` RPC
```sql
LEFT JOIN LATERAL (SELECT COUNT(*)::int FROM leads WHERE tenant_id = t.id) lc ON true
LEFT JOIN LATERAL (SELECT COUNT(*)::int FROM profiles WHERE tenant_id = t.id) pc ON true
LEFT JOIN LATERAL (SELECT COUNT(*)::int FROM clientes WHERE tenant_id = t.id) cc ON true
```
- **Complexidade**: O(tenants √ó 3 COUNTs) por p√°gina
- **Limite**: Com 1000 tenants, mesmo paginado, os COUNTs individuais s√£o caros
- **Mitiga√ß√£o futura**: Counter caching ou approximate COUNT

### Gargalo 3: Realtime Fanout
```
wa_conversations UPDATE ‚Üí Realtime ‚Üí N canais (1 por usu√°rio com Inbox aberto)
```
- **Throughput**: Cada update gera N notifica√ß√µes WebSocket
- **Limite**: Com 100 usu√°rios no Inbox = 100 notifica√ß√µes por mensagem
- **Mitiga√ß√£o existente**: Filtro por `tenant_id` no canal reduz fanout cross-tenant

### Gargalo 4: Supabase Auth Token Refresh
- **Mecanismo**: JWT expira a cada hora. Refresh token √© usado automaticamente.
- **Limite**: Em opera√ß√µes batch (sync, upload m√∫ltiplo), token pode expirar mid-operation
- **Mitiga√ß√£o existente**: `refreshSession()` antes de uploads cr√≠ticos

---

## 4.4 ‚Äî Limites Te√≥ricos vs Reais

| Dimens√£o | Te√≥rico (Postgres) | Real (Supabase Pro) | Real (Este Sistema) |
|----------|-------------------|--------------------|--------------------|
| Connections | 10.000+ | ~500 (PgBouncer) | ~200 (pool sharing) |
| Rows/table | Bilh√µes | Bilh√µes | ~10M antes de degrada√ß√£o (sem parti√ß√£o) |
| Queries/sec | 100.000+ | ~5.000 (estimado) | ~1.000 (RLS overhead) |
| Write throughput | 50.000 TPS | ~2.000 TPS | ~500 TPS (triggers + RLS + audit) |
| Realtime events/sec | N/A | ~1.000 | ~200 (fanout √ó channels) |
